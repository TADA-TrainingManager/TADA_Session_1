{
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "colab": {
      "name": "Format&Plot_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Preliminary Step\n",
        "\n",
        "# From: https://www.geeksforgeeks.org/python-dictionary/\n",
        "# Create a dictionary with integer keys\n",
        "Dict = {1: 'Geeks', 2: 'For', 3: 'Geeks'}\n",
        "print(\"\\nDictionary with the use of Integer Keys: \")\n",
        "print(Dict)\n",
        " \n",
        "# Create a dictionary with mixed keys\n",
        "Dict = {'Name': 'Geeks', 1: [1, 2, 3, 4]}\n",
        "print(\"\\nDictionary with the use of Mixed Keys: \")\n",
        "print(Dict)"
      ],
      "metadata": {
        "id": "T1MqK80g7WH9"
      },
      "id": "T1MqK80g7WH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Mounting The Google Drive. User will need to give access to their Google drive for reading/writing data.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yPLKITeVmbEH"
      },
      "id": "yPLKITeVmbEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 - ensuring the needed backends are installed.\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install basemap\n",
        "!pip install h5py\n",
        "!pip install requests"
      ],
      "metadata": {
        "trusted": true,
        "id": "1f066536-8b5d-4b15-bcf2-8959ba50b72d"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1f066536-8b5d-4b15-bcf2-8959ba50b72d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 - Importing the backends.\n",
        "\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.dates import DayLocator, HourLocator, DateFormatter\n",
        "from matplotlib.colors import LogNorm\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "from google.colab import files\n",
        "import h5py\n",
        "from getpass import getpass\n",
        "from http.client import NOT_FOUND, UNAUTHORIZED\n",
        "from pathlib import Path\n",
        "from requests import Session"
      ],
      "metadata": {
        "trusted": true,
        "id": "3d8f415e-0157-4755-9f66-c5cb973a4291"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3d8f415e-0157-4755-9f66-c5cb973a4291"
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Step: Download ACTIVATE data programatically (https://forum.earthdata.nasa.gov/viewtopic.php?t=2330&sid=16db867810f19249101e706e58b75cd9).\n",
        "\n",
        "url_to_path = lambda url, output_dir: output_dir.joinpath(url.split('/')[-1])\n",
        "colab_path = \"/content/drive/MyDrive/Colab_Notebooks/TADA_Session_1-main/\" # colab session directory\n",
        "\n",
        "print(\"Welcome to the ASDC Download Script!\\nThis script downloads data from https://asdc.larc.nasa.gov/data/\")\n",
        "with Session() as session:\n",
        "    # get login\n",
        "    url = input(\"Enter the top level URL (you can also enter 'test' to download a small dataset)\\n\\turl: \")\n",
        "    if url == \"test\":\n",
        "        url = \"https://asdc.larc.nasa.gov/data/AJAX/CH2O_1/\"\n",
        "    token = getpass(\"Enter your token, if you don't have a token, get one from https://urs.earthdata.nasa.gov/\\n\\ttoken: \")\n",
        "    if not token:\n",
        "        print(\"Token cannot be blank, exiting.\")\n",
        "        exit()\n",
        "    session.headers = {\"Authorization\": f\"Bearer {token}\"}\\\n",
        "\n",
        "    # verify login works\n",
        "    response = session.get(url)\n",
        "    if not response.ok:\n",
        "        if response.status_code == UNAUTHORIZED:\n",
        "            print(f\"Earthdata Login reponded with Unauthorized, did you enter a valid token?\")\n",
        "            exit()\n",
        "        if response.status_code == NOT_FOUND:\n",
        "            print(f\"The top level URL does not exist, select a URL within https://asdc.larc.nasa.gov/data/\")\n",
        "            exit()\n",
        "    output_dir = Path(colab_path)\n",
        "\n",
        "    # get a list of all urls\n",
        "    pages = [url]\n",
        "    file_urls = []\n",
        "    print(\"Getting file links\")\n",
        "    for i, page in enumerate(pages):\n",
        "        print(f\"Checking {page} for links\", end=\"\\r\", flush=True)\n",
        "        response = session.get(page)\n",
        "        if not response.ok:\n",
        "            if response.status_code == NOT_FOUND:\n",
        "                print(f\"The follwoing page was not found: {url}\")\n",
        "            else:\n",
        "                print(f\"Recieved {response.reason} status for {page}\")\n",
        "        content = response.content.decode('utf-8')\n",
        "        if '<table id=\"indexlist\">' not in content:\n",
        "            print(f\"Data table not found for {page}\")\n",
        "            continue\n",
        "        table_content = content.split('<table id=\"indexlist\">')[-1].split('</table>')[0]\n",
        "        hrefs = {part.split('\"')[0] for i, part in enumerate(table_content.split('href=\"')) if i}\n",
        "        for href in hrefs:\n",
        "            if href.endswith('/'):\n",
        "                pages.append(page + href)\n",
        "            else:\n",
        "                file_urls.append(page + href)\n",
        "    if not file_urls:\n",
        "        print(\"No files found, exiting.\")\n",
        "        exit()\n",
        "    \n",
        "    # offer to remove existing data\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    if output_dir.exists() and len(list(output_dir.iterdir())):\n",
        "        if input(f\"There's already data in {output_dir.absolute()}, \\n\\tRemove it? [y/n]: \") == \"y\":\n",
        "            for path in output_dir.iterdir():\n",
        "                path.unlink()\n",
        "\n",
        "    # get a list of new files (ignore already download files if the size is the same)\n",
        "    print(\"Getting size\")\n",
        "    total_size = 0\n",
        "    file_count = len(file_urls)\n",
        "    new_files = []\n",
        "    for i, url in enumerate(file_urls):\n",
        "        print(f\"Getting size for file {i+1} of {file_count}\", end=\"\\r\", flush=True)\n",
        "        _response = session.head(url)\n",
        "        if url_to_path(url, output_dir).exists() and _response.headers.get('content-length') != url_to_path(url, output_dir).stat().st_size:\n",
        "            continue\n",
        "        total_size += int(_response.headers.get('content-length'))\n",
        "        new_files.append(url)\n",
        "    if not new_files:\n",
        "        print(\"No new files, exiting.\")\n",
        "        exit()\n",
        "    if input(f\"Found {len(new_files)} files totaling {total_size // 1024**2} MB in {output_dir.absolute()}.\\n\\tDownload [y/n]: \") == 'n':\n",
        "        exit()\n",
        "    \n",
        "    # downlaod files\n",
        "    for i, url in enumerate(new_files):\n",
        "        print(f\"Downloading file {i+1} of {len(file_urls)}\", end=\"\\r\", flush=True)\n",
        "        _response = session.get(url)\n",
        "        with url_to_path(url, output_dir).open('wb') as file:\n",
        "            file.write(_response._content)\n",
        "    print(\"\\nDownload Complete\")"
      ],
      "metadata": {
        "id": "lBkDq2qugWmL"
      },
      "id": "lBkDq2qugWmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 - Defining the prodedure to convert data from ICT standard files to python dictionaries.\n",
        "\n",
        "#########################################################################################################################\n",
        "# \n",
        "# Description: Procedure to open ICT file and format the data and headers into the output (output_dictionary) python \n",
        "# dictionary. Each of the required inputs for this code are described as follows:\n",
        "# 1) filename: String or character containing the name of the desired ICT (*.ict) file.\n",
        "#\n",
        "# the output of this code is output_dictionary, which is a python3 dictionary containing column-arrays for each of the\n",
        "# parameters in the *.ict file\n",
        "# -> each column corresponds to a line in the provided *.ict file\n",
        "#\n",
        "#   EXAMPLES:\n",
        "#       output_dictionary = importICT.imp(\"activate-mrg1_hu25_20200214_R0.ict\")\n",
        "#\n",
        "#       print(output_dictionary)\n",
        "#\n",
        "#       output_dictionary =\n",
        "#\n",
        "#           {'Time_Start_seconds': array([61301., 61302., 61303., ..., 72258., 72259., 72260.]), \n",
        "#           'Time_Stop_seconds': array([61302., 61303., 61304., ..., 72259., 72260., 72261.]), \n",
        "#           'Latitude_THORNHILL_ deg': array([37.085528, 37.085798, 37.086065, ..., 37.126424, 37.126694, ...\n",
        "#\n",
        "#########################################################################################################################\n",
        "\n",
        "def imp(filename = None): \n",
        "    G = open(filename, 'r') # open *.ict file\n",
        "    g = G.readlines() # read *.ict file\n",
        "    DATEinfo = np.array(g[6].split(\",\")) \n",
        "    DATE = DATEinfo[0:3] # save date to add to file\n",
        "\n",
        "    Fv = g[11]  # locate line with fill values, wich are located on line 11 of .ict file, to replace with 'nan'\n",
        "    fv = Fv.split(\",\") # create array of fill values\n",
        "    varend = int(g[9]) # locate line with number of variables, which is located on line 9 of *.ict file\n",
        "\n",
        "    full_var_titles = [\"\" for x in range(np.add(varend,1))] # create empty string array for full variable titles\n",
        "\n",
        "    # fill full variable title string array with full variable titles starting on line 12 of *.ict file\n",
        "    i2 = 12\n",
        "    for i1 in range(varend):\n",
        "        full_var_titles[np.add(i1,1)] = g[i2]\n",
        "        i2 = np.add(i2,1)\n",
        "\n",
        "    # fill first variable title string array position with full variable title of first variable on line 8 of *.ict file    \n",
        "    starttime = g[8]\n",
        "    full_var_titles[0] = starttime[0:len(starttime)-1]    \n",
        "\n",
        "    # locate data header and data start rows of *.ict file using line 0 of *.ict file\n",
        "    st = g[0]\n",
        "    Vr_id = np.array(st.split(\",\"))\n",
        "    vr_id = np.add(int(Vr_id[0]),1)\n",
        "\n",
        "    rawdata = g[vr_id::] # create raw data array starting at data start row\n",
        "\n",
        "    # create empty string arrays for final variable names, units, and extra info\n",
        "    var_names = [\"\" for x in range(len(full_var_titles))]\n",
        "    var_units = [\"\" for x in range(len(full_var_titles))]\n",
        "    \n",
        "    # iteratively fill string arrays with final variable names, units, and extra info\n",
        "    for i1 in np.arange(0,len(full_var_titles)).reshape(-1):\n",
        "        fvt = full_var_titles[i1]\n",
        "        FVT = fvt.split(\",\")\n",
        "        var_names[i1] = FVT[0]\n",
        "        var_units[i1] = FVT[1]\n",
        "\n",
        "    # create data arraw with length of dataset and width of number of variables       \n",
        "    data = np.zeros((len(rawdata),len(full_var_titles)))\n",
        "\n",
        "    # iteratively fill data array with data and replace fv values with NAN\n",
        "    for i1 in np.arange(0,len(rawdata)).reshape(-1):\n",
        "        processdata1 = rawdata[i1]\n",
        "        processdata2 = processdata1.split(\",\")\n",
        "        for i2 in np.arange(0,len(processdata2)).reshape(-1):\n",
        "          if(len(processdata2[i2]) == 1):\n",
        "            data[i1,i2] = 0\n",
        "          else:\n",
        "            if i1 > 1:\n",
        "              if processdata2[i2] == fv[np.add(i2,- 1)]:\n",
        "                processdata2[i2] = 'nan'\n",
        "              elif float(processdata2[i2]) <= -999.0:\n",
        "                processdata2[i2] = 'nan'\n",
        "            data[i1,i2] = processdata2[i2]     \n",
        "    \n",
        "    # creat empty dictionary\n",
        "    output_dictionary = {}\n",
        "\n",
        "    # fill dictionary with data and keys   \n",
        "    for i1 in range(len(full_var_titles)):\n",
        "        output_dictionary[(\"%s_%s\"%(var_names[i1],var_units[i1]))] = data[:,i1]\n",
        "\n",
        "    output_dictionary[\"date\"] = DATE # Add date to dictionary  \n",
        "    start_time_array = data[:,0] # select start time\n",
        "    end_time_array = data[:,1] # select end time\n",
        "    mid_time_array = np.array(np.mean((start_time_array,end_time_array),axis=0))# create midtime array\n",
        "    start_frmttimedata = [\"\" for x in range(len(start_time_array))] # create array of zeros for start datetime data\n",
        "    start_mattimedata = np.zeros((len(start_time_array),6)) # create array of zeros for start datetime data\n",
        "    end_frmttimedata = [\"\" for x in range(len(end_time_array))] # create array of zeros for end datetime data\n",
        "    end_mattimedata = np.zeros((len(end_time_array),6)) # create array of zeros for end datetime data\n",
        "    mid_frmttimedata = [\"\" for x in range(len(end_time_array))] # create array of zeros for mid datetime data\n",
        "    mid_mattimedata = np.zeros((len(end_time_array),6)) # create array of zeros for mid datetime data\n",
        "    # fill empty arrays formated datetime and matix date time\n",
        "    for i1 in range(len(start_time_array)):   \n",
        "        # convert DATE to separate integers\n",
        "        Yr = int(DATE[0])\n",
        "        Mon = int(DATE[1])\n",
        "        Day = int(DATE[2])\n",
        "                \n",
        "        # convert seconds after mindnight to hour, minute, and second integers for start times\n",
        "        Hr = int(np.floor(start_time_array[i1]/(60*60))) \n",
        "        Mnt = int(np.floor((start_time_array[i1]/(60*60)-np.floor(start_time_array[i1]/(60*60)))*60))\n",
        "        Secd = int(((start_time_array[i1]/(60*60)-np.floor(start_time_array[i1]/(60*60)))*60-\n",
        "          np.floor((start_time_array[i1]/(60*60)-np.floor(start_time_array[i1]/(60*60)))*60))*60)\n",
        "\n",
        "        # set day to next dat if hours are greater than 23\n",
        "        if i1 > 0:\n",
        "           if start_time_array[i1] - start_time_array[i1-1] < 0:\n",
        "               Day = Day + 1\n",   
        "        start_mattimedata[i1,:]  = [Yr,Mon,Day,Hr,Mnt,Secd] # store the matrix of year, month, day, hour, minute, second \n",
        "        start_frmttimedata[i1] = datetime.datetime(Yr,Mon,Day,Hr,Mnt,Secd) # store the formatted datetime\n",
        "\n",
        "        # convert seconds after mindnight to hour, minute, and second integers for end times\n",
        "        Hr = int(np.floor(end_time_array[i1]/(60*60))) \n",
        "        Mnt = int(np.floor((end_time_array[i1]/(60*60)-np.floor(end_time_array[i1]/(60*60)))*60))\n",
        "        Secd = int(((end_time_array[i1]/(60*60)-np.floor(end_time_array[i1]/(60*60)))*60-\n",
        "          np.floor((end_time_array[i1]/(60*60)-np.floor(end_time_array[i1]/(60*60)))*60))*60)\n",
        "\n",  
        "        end_mattimedata[i1,:]  = [Yr,Mon,Day,Hr,Mnt,Secd] # store the matrix of year, month, day, hour, minute, second \n",
        "        end_frmttimedata[i1] = datetime.datetime(Yr,Mon,Day,Hr,Mnt,Secd) # store the formatted datetime\n",
        "\n",
        "        # convert seconds after mindnight hour, minute, and second integers for start times\n",
        "        Hr = int(np.floor(mid_time_array[i1]/(60*60))) \n",
        "        Mnt = int(np.floor((mid_time_array[i1]/(60*60)-np.floor(mid_time_array[i1]/(60*60)))*60))\n",
        "        Secd = int(((mid_time_array[i1]/(60*60)-np.floor(mid_time_array[i1]/(60*60)))*60-\n",
        "          np.floor((mid_time_array[i1]/(60*60)-np.floor(mid_time_array[i1]/(60*60)))*60))*60)\n",
        "\n",
        "        mid_mattimedata[i1,:]  = [Yr,Mon,Day,Hr,Mnt,Secd] # store the matrix of year, month, day, hour, minute, second \n",
        "        mid_frmttimedata[i1] = datetime.datetime(Yr,Mon,Day,Hr,Mnt,Secd) # store the formatted datetime\n",
        "\n",
        "    # Add start, mid, and end frmttimedata and mattimedata to dictionary   \n",
        "    output_dictionary[\"start_fmtdatetime\"] = start_frmttimedata\n",
        "    output_dictionary[\"start_mattimedata\"] = start_mattimedata\n",
        "    output_dictionary[\"mid_fmtdatetime\"] = mid_frmttimedata\n",
        "    output_dictionary[\"mid_matdatetime\"] = mid_mattimedata    \n",
        "    output_dictionary[\"end_fmtdatetime\"] = end_frmttimedata\n",
        "    output_dictionary[\"end_matdatetime\"] = end_mattimedata\n",
        "    G.close() # close data file      \n",
        "    return output_dictionary\n",
        "    ##\n",
        "    return output_dictionary"
      ],
      "metadata": {
        "trusted": true,
        "id": "7b086e54-ba8c-4c5a-aca6-7ad8d3fb8b3c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7b086e54-ba8c-4c5a-aca6-7ad8d3fb8b3c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 - Running the ICT conversion procedured defined in Step 4 to get the 60s in-situ data and the leg index files for\n",
        "# a case study research flight. Displaying all of the parameters that avarailable in each newly created dictionary\n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for converting the data and leg index files from their *.ict format to a python \n",
        "# dictionary and displaying all parameters that are in the newly created dictionary.\n",
        "# \n",
        "#########################################################################################################################\n",
        "\n",
        "colab_path = \"/content/drive/MyDrive/Colab_Notebooks/TADA_Session_1-main/\" # colab session directory\n",
        "\n",
        "data_filename = \"ACTIVATE-MRG60-HU25_MERGE_20200828_R0.ict\" # data filename\n",
        "LegIndex_filename = \"ACTIVATE-LEGFLAGS_HU25_20200828_R1.ict\" # data filename\n",
        "\n",
        "data_pathfilename = \"%s%s\"%(colab_path,data_filename) # data path and filename\n",
        "LegIndex_pathfilename = \"%s%s\"%(colab_path,LegIndex_filename) # leg index path and filename\n",
        "\n",
        "# convert the *.ict files into python dictionaries\n",
        "data_dictionary = imp(data_pathfilename)\n",
        "LegID_dictionary = imp(LegIndex_pathfilename)\n",
        "\n",
        "# display the avalable parameters from each dictionary\n",
        "for key in data_dictionary.items():\n",
        "    print(key[0])\n",
        "print(\"\")    \n",
        "for key in LegID_dictionary.items():\n",
        "    print(key[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "4535f601-081a-415d-bf12-749f2ddc011b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4535f601-081a-415d-bf12-749f2ddc011b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 - Retrieving the standard acronmym identifiers from the provided LegTypes.csv file and adding the leg types to\n",
        "# the dictonary containing the leg inedx information. The LegTypes.csv file comes from line 30 of the *.ict LegIndex file \n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for converting the numeric leg indicies to acronym identifiers\n",
        "# \n",
        "#########################################################################################################################\n",
        "\n",
        "# generate arrays for numeric leg indicies \n",
        "LegIndex = np.array([value for key, value in LegID_dictionary.items() if key.startswith(\"LegIndex\")],dtype=int) \n",
        "LegIndex = LegIndex[0]\n",
        "# read LegTypes.csv to match leg the leg index to the leg type (see line)\n",
        "legtype = np.array(pd.read_csv (\"%sLegTypes.csv\"%colab_path))\n",
        "\n",
        "# create empty string array to store leg types and loop through LegIndex file and assign types\n",
        "LegType = [\"\" for x in range(len(LegIndex))] \n",
        "for i1 in range(len(LegIndex)):\n",
        "  if int(str(LegIndex[i1])) > 0: \n",
        "    a = int(str(LegIndex[i1])[7:9])\n",
        "    idx = np.array(np.where(legtype[:,0]==a))\n",
        "    LegType[i1] = legtype[idx,1][0][0]\n",
        "\n",
        "# store the leg type array to the leg index dictionary\n",
        "LegID_dictionary[\"LegType\"] = LegType"
      ],
      "metadata": {
        "id": "olEUpfzyC7tH"
      },
      "id": "olEUpfzyC7tH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 - Gerating basic in-situ research flight track and aircraft altitude time series.\n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for plotting a flight track and time series of measured altitude colored by aerosol particle \n",
        "# surface area concentration (SA) annotated with the leg identifiers.\n",
        "#\n",
        "#########################################################################################################################\n",
        "\n",
        "# generate arrays for latitude, longitude, altitude, formatted time, aerosol particle surface area concentration, and  \n",
        "# date from the data dictionary\n",
        "latitude_data = np.array([value for key, value in data_dictionary.items() if key.startswith(\"Latitude\")])[0]\n",
        "longitude_data = np.array([value for key, value in data_dictionary.items() if key.startswith(\"Longitude\")])[0]\n",
        "altitude_data = np.array([value for key, value in data_dictionary.items() if key.startswith(\"GPS_Altitude\")])[0]\n",
        "time_data = np.array([value for key, value in data_dictionary.items() if key.startswith(\"mid_fmtdatetime\")])[0]\n",
        "SA_data = np.array([value for key, value in data_dictionary.items() if key.startswith(\"IntegS_Dopt90to7500nm\")])[0]\n",
        "\n",
        "SA_data[np.isnan(SA_data)] = 0\n",
        "date = np.array([value for key, value in data_dictionary.items() if key.startswith(\"date\")])[0]\n",
        "\n",
        "# generate arrays for the leg types and the formatted start, midpoint, and end times of each leg\n",
        "LegTypes = np.array([value for key, value in LegID_dictionary.items() if key.startswith(\"LegType\")])[0]\n",
        "LegTypes_starttime = np.array([value for key, value in LegID_dictionary.items() if key.startswith(\"start_fmtdatetime\")])[0] \n",
        "LegTypes_midtime = np.array([value for key, value in LegID_dictionary.items() if key.startswith(\"mid_fmtdatetime\")])[0]\n",
        "LegTypes_endtime = np.array([value for key, value in LegID_dictionary.items() if key.startswith(\"end_fmtdatetime\")])[0]\n",
        "\n",
        "plt.rcParams.update({'font.size': 22}) # set default font size for this and subsequent figures 0\n",
        "fig,ax1=plt.subplots(1, 1,figsize=(24,12)) # create figure and subplot\n",
        "#ax1.plot(time_data, altitude_data/1000,'-ro',lw=2) # plotting time versus aircraft altitude, in km  \n",
        "sc = ax1.scatter(time_data, altitude_data/1000,c=SA_data, cmap='jet')\n",
        "cbar = fig.colorbar(sc, ax=ax1)\n",
        "cbar.ax.get_yaxis().labelpad = 50\n",
        "cbar.set_label(r'Surface Area Concentration (um$^{2}$ cm$^{-3}$)', rotation=270)\n",
        "\n",
        "# loop through all legs indicies and annotate the time series with the leg type \n",
        "# as defined by the last two digits of the leg index\n",
        "for i1 in range(len(LegTypes)):\n",
        "  idx = np.array(np.where((time_data>=LegTypes_starttime[i1])&(time_data<=LegTypes_endtime[i1])))[0]\n",
        "  leg_altitude = np.nanmean(altitude_data[idx])\n",
        "  LegType_text = \"%s\"%LegTypes[i1]\n",
        "  t = ax1.text(LegTypes_midtime[i1], leg_altitude/1000,LegType_text,\n",
        "               horizontalalignment ='center',verticalalignment ='bottom',fontsize = 10)\n",
        "\n",
        "# format axis\n",
        "ax1.set_ylim(0,None) # cut y-axis off at zero\n",
        "# set the line widths of the axes\n",
        "for axis in ['top','bottom','left','right']:\n",
        "    ax1.spines[axis].set_linewidth(1.5)\n",
        "ax1.tick_params(direction='in', length=6, width=1.5) # set inside facing ticks, ticklength, and tick line width\n",
        "ax1.xaxis.set_major_formatter(DateFormatter('%H:%M'))  # set xaxis major lines datetime format\n",
        "ax1.fmt_xdata = DateFormatter('%H:%M') # set xaxis datetime format\n",
        "ax1.set_xlabel(\"Time (hh:mm, UTC)\") # set xaxis label \n",
        "ax1.set_ylabel(\"Altitude (km)\") # set yaxis label \n",
        "plt.gcf().autofmt_xdate() # autoformat datetimes to look better\n",
        "\n",
        "ax1.set_title(\"Date: %s-%s-%s\"%(date[0],date[1],date[2])) #set title as flight date.\n",
        "\n",
        "# display and save figure using the *.ict data filename \n",
        "plt.savefig(\"%saltitude_SA_timeseries_%s-%s-%s\"%(colab_path,date[0],date[1],date[2]), dpi=300)\n",
        "plt.show() # function to display the plot\n",
        "\n",
        "print(\"\") # blank space for separation\n",
        "\n",
        "# create figure a geographical map for plotting flight tracks\n",
        "ax2=plt.subplots(1, 1,figsize=(12,12))\n",
        "\n",
        "# use Basemap function to display map of the research flight's study region and format\n",
        "m = Basemap(projection='cyl',llcrnrlat=np.floor(np.nanmin(latitude_data)),urcrnrlat=np.ceil(np.nanmax(latitude_data)),\\\n",
        "            llcrnrlon=np.floor(np.nanmin(longitude_data)),urcrnrlon=np.ceil(np.nanmax(longitude_data)),resolution='c')\n",
        "m.drawcoastlines() # diplay coastlines \n",
        "m.fillcontinents() # fill continents with grey \n",
        "\n",
        "# plot the flight track in latitude and longitude coordinates\n",
        "#x,y = m(longitude_data,latitude_data)\n",
        "#m.plot(x,y, '-ro')\n",
        "# plot the flight track in latitude and longitude coordinates\n",
        "x,y = m(longitude_data,latitude_data)\n",
        "m.scatter(x,y,c=SA_data, cmap='jet')\n",
        "cbar = fig.colorbar(sc, ax=ax1)\n",
        "cbar.ax.get_yaxis().labelpad = 50\n",
        "cbar.set_label(r'SA (um$^{2}$ cm$^{-3}$)', rotation=270)\n",
        "\n",
        "# draw parallels and meridians\n",
        "m.drawparallels(np.arange(np.floor(np.nanmin(latitude_data)),np.ceil(np.nanmax(latitude_data)),1),labels=[1,1,0,1])\n",
        "m.drawmeridians(np.arange(np.floor(np.nanmin(longitude_data)),np.ceil(np.nanmax(longitude_data)),2),labels=[1,1,0,1])\n",
        "\n",
        "m.drawmapboundary(fill_color='#FFFFFF') # draw box around plot\n",
        "\n",
        "# format axes\n",
        "plt.xlabel('Longitude', labelpad=40)\n",
        "plt.ylabel('Latitude', labelpad=80)\n",
        "\n",
        "plt.title(\"Date: %s-%s-%s\"%(date[0],date[1],date[2])) #set title as flight date.\n",
        "plt.savefig(\"%sSA_flight_track_%s-%s-%s\"%(colab_path,date[0],date[1],date[2]), dpi=300)\n",
        "plt.show() # function to show the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "3903a554-9bc3-40f4-958d-b46661a4a798"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3903a554-9bc3-40f4-958d-b46661a4a798"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8 - Reading the HSRL and RSP data into python dictionaries.\n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for reading the HSRL-2 and RSP data files from their *.h5 format into a python dictionary and \n",
        "# displaying all parameters that are in the newly created dictionary.\n",
        "# \n",
        "#########################################################################################################################\n",
        "\n",
        "# read the RSP data and display all avalable data products\n",
        "RSP_dictionary = h5py.File(\"%sACTIVATE-RSP-AER_UC12_20200828_R1.h5\"%colab_path, \"r\")\n",
        "print(\"RSP data products:\")\n",
        "print(RSP_dictionary.keys())\n",
        "\n",
        "# take the flight date from the RSP readme line 9\n",
        "RSP_Date = [str(RSP_dictionary[\"000-README\"][9])[24:28],str(RSP_dictionary[\"000-README\"][9])[28:30],str(RSP_dictionary[\"000-README\"][9])[30:32]]\n",
        "\n",
        "rsp_time_array = np.array(RSP_dictionary[\"time\"]) # select rsp time\n",
        "rsp_frmttimedata = [\"\" for x in range(len(rsp_time_array))] # create array of zeros for rsp datetime data\n",
        "rsp_mattimedata = np.zeros((len(rsp_time_array),6)) # create array of zeros for start datetime data\n",
        "# fill empty arrays formated datetime and matix date time\n",
        "for i1 in range(len(rsp_time_array)):   \n",
        "    # convert DATE to separate integers\n",
        "    Yr = int(RSP_Date[0])\n",
        "    Mon = int(RSP_Date[1])\n",
        "    Day = int(RSP_Date[2])\n",
        "                \n",
        "    # fractional hours after midnight to hour, minute, and second integers for rsp times\n",
        "    Hr = int(np.floor(rsp_time_array[i1])) \n",
        "    Mnt = int(np.floor((rsp_time_array[i1]-np.floor(rsp_time_array[i1]))*60))\n",
        "    Secd = int(((rsp_time_array[i1]-np.floor(rsp_time_array[i1]))*60-\n",
        "    np.floor((rsp_time_array[i1]-np.floor(rsp_time_array[i1]))*60))*60)\n",
        "\n",
        "    # set day to next dat if hours are greater than 23\n",
        "    if Hr > 23:\n",
        "        Hr = Hr - 24\n",
        "        Day = Day + 1\n",
        "\n",
        "    rsp_mattimedata[i1,:]  = [Yr,Mon,Day,Hr,Mnt,Secd] # store the matrix of year, month, day, hour, minute, second \n",
        "    rsp_frmttimedata[i1] = datetime.datetime(Yr,Mon,Day,Hr,Mnt,Secd) # store the formatted datetime\n",
        "\n",
        "# read the HSRL-2 data and display all groups and avalable data products\n",
        "HSRL2_dictionary = h5py.File(\"%sACTIVATE-HSRL2_UC12_20200828_R1.h5\"%colab_path, \"r\")\n",
        "print(\"HSRL-2 metadata:\")\n",
        "print(HSRL2_dictionary.keys())\n",
        "print(\"HSRL-2 data products:\")\n",
        "print(HSRL2_dictionary[\"DataProducts\"].keys())\n",
        "print(\"HSRL-2 navigation and time data:\")\n",
        "print(HSRL2_dictionary[\"Nav_Data\"].keys())\n",
        "\n",
        "# take the flight date from the HSRL readme line 4\n",
        "HSRL2_Date = [str(HSRL2_dictionary['000_Readme'][4])[2:6],str(HSRL2_dictionary['000_Readme'][4])[7:9],str(HSRL2_dictionary['000_Readme'][4])[10:12]]\n",
        "\n",
        "hsrl2_time_array = np.array(HSRL2_dictionary[\"Nav_Data\"][\"gps_time\"])# select hsrl-2 time\n",
        "hsrl2_frmttimedata = [\"\" for x in range(len(hsrl2_time_array))] # create array of zeros for hsrl-2 datetime data\n",
        "hsrl2_mattimedata = np.zeros((len(hsrl2_time_array),6)) # create array of zeros for start datetime data\n",
        "\n",
        "# fill empty arrays formated datetime and matix date time\n",
        "for i1 in range(len(hsrl2_time_array)):   \n",
        "    # convert DATE to separate integers\n",
        "    Yr = int(HSRL2_Date[0])\n",
        "    Mon = int(HSRL2_Date[1])\n",
        "    Day = int(HSRL2_Date[2])\n",
        "                \n",
        "    # convert fractional hours after midnight to hour, minute, and second integers for hsrl-2 times\n",
        "    Hr = int(np.floor(hsrl2_time_array[i1])) \n",
        "    Mnt = int(np.floor((hsrl2_time_array[i1]-np.floor(hsrl2_time_array[i1]))*60))\n",
        "    Secd = int(((hsrl2_time_array[i1]-np.floor(hsrl2_time_array[i1]))*60-\n",
        "    np.floor((hsrl2_time_array[i1]-np.floor(hsrl2_time_array[i1]))*60))*60)\n",
        "\n",
        "    # set day to next dat if hours are greater than 23\n",
        "    if Hr > 23:\n",
        "        Hr = Hr - 24\n",
        "        Day = Day + 1\n",
        "\n",
        "    hsrl2_mattimedata[i1,:]  = [Yr,Mon,Day,Hr,Mnt,Secd] # store the matrix of year, month, day, hour, minute, second \n",
        "    hsrl2_frmttimedata[i1] = datetime.datetime(Yr,Mon,Day,Hr,Mnt,Secd) # store the formatted datetime\n"
      ],
      "metadata": {
        "id": "5e2db540-da1b-4f8b-9ed3-fd1fcfe7989f"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5e2db540-da1b-4f8b-9ed3-fd1fcfe7989f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9 - Generating HSRL-2 particle exctinction coefficient curtain time series and time series of HSRL-2 and RSP total  \n",
        "# aerosol optical thickness (AOT). \n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for plotting HSRL-2 aerosol particle exctinction coefficient curtain time series and time series \n",
        "# of HSRL-2 and RSP total AOT measured at 532 nm wavelngths.\n",
        "#\n",
        "#########################################################################################################################\n",
        "\n",
        "# generate arrays for the HSRL-2 altitude grid and aerosol particle exctinction coefficient HSRL-2 dictionary\n",
        "hsrl2_altitudegrid_data = np.array(HSRL2_dictionary[\"DataProducts\"][\"Altitude\"])[0] # m\n",
        "hsrl2_extinction_data = np.array(HSRL2_dictionary[\"DataProducts\"][\"532_ext\"]) # km-1\n",
        "\n",
        "# generate arrays for the HSRL-2 and RSP 532 nm total AOT\n",
        "hsrl2_aot_data = np.array(HSRL2_dictionary[\"DataProducts\"][\"532_AOT_hi\"])\n",
        "rsp_aot_data = np.add(np.array(RSP_dictionary[\"aerosol_tau_c\"])[:,8],np.array(RSP_dictionary[\"aerosol_tau_f\"])[:,8])\n",
        "\n",
        "fig,ax1=plt.subplots(1, 1,figsize=(24,12)) # create figure and subplot\n",
        "ax1.set_facecolor('gray')\n",
        "\n",
        "# plotting a color plot of exctinction coefficient, in km-1 with time versus aircraft altitude, in km  \n",
        "Z = hsrl2_extinction_data\n",
        "X = hsrl2_frmttimedata\n",
        "Y = hsrl2_altitudegrid_data/1000\n",
        "c = ax1.pcolor(X, Y, Z.T, cmap='jet', norm=LogNorm(vmin=0.01, vmax=1))\n",
        "cbar = fig.colorbar(c, ax=ax1)\n",
        "cbar.ax.get_yaxis().labelpad = 50\n",
        "cbar.set_label(r'Extinction Coefficient (km$^{-1}$)', rotation=270)\n",
        "\n",
        "# format axis\n",
        "ax1.set_ylim(0,None) # cut y-axis off at zero\n",
        "# set the line widths of the axes\n",
        "for axis in ['top','bottom','left','right']:\n",
        "    ax1.spines[axis].set_linewidth(1.5)\n",
        "ax1.tick_params(direction='in', length=6, width=1.5) # set inside facing ticks, ticklength, and tick line width\n",
        "ax1.xaxis.set_major_formatter(DateFormatter('%H:%M'))  # set xaxis major lines datetime format\n",
        "ax1.fmt_xdata = DateFormatter('%H:%M') # set xaxis datetime format\n",
        "ax1.set_xlabel(\"Time (hh:mm, UTC)\") # set xaxis label \n",
        "ax1.set_ylabel(\"Altitude (km)\") # set yaxis label \n",
        "plt.gcf().autofmt_xdate() # autoformat datetimes to look better\n",
        "\n",
        "ax1.set_title(\"Date: %s-%s-%s\"%(HSRL2_Date[0],HSRL2_Date[1],HSRL2_Date[2])) #set title as flight date.\n",
        "\n",
        "# display and save figure using the *.ict data filename \n",
        "plt.savefig(\"%sHSRL-2_532nmAerosolExtCoef_pcolortimeseries_%s-%s-%s\"%(colab_path,HSRL2_Date[0],HSRL2_Date[1],HSRL2_Date[2]), dpi=300)\n",
        "plt.show() # function to display the plot\n",
        "\n",
        "print(\"\") # blank space for separation\n",
        "\n",
        "fig,ax2=plt.subplots(1, 1,figsize=(24,12)) # create figure and subplot\n",
        "ax2.plot(hsrl2_frmttimedata, hsrl2_aot_data,'-ro',lw=2,label='HSRL-2') # plotting hsrl-2 time versus 532 nm AOT\n",
        "ax2.plot(rsp_frmttimedata, rsp_aot_data,'-bd',lw=2,label='RSP') # plotting rsp time versus 532 nm AOT\n",
        "\n",
        "ax2.legend(); # add legend\n",
        "ax2.set_ylim(0,None) # cut y-axis off at zero\n",
        "\n",
        "# set the line widths of the axes\n",
        "for axis in ['top','bottom','left','right']:\n",
        "    ax2.spines[axis].set_linewidth(1.5)\n",
        "\n",
        "ax2.tick_params(direction='in', length=6, width=1.5) # set inside facing ticks, ticklength, and tick line width\n",
        "ax2.xaxis.set_major_formatter(DateFormatter('%H:%M'))  # set xaxis major lines datetime format\n",
        "ax2.fmt_xdata = DateFormatter('%H:%M') # set xaxis datetime format\n",
        "ax2.set_xlabel(\"Time (hh:mm, UTC)\") # set xaxis label \n",
        "ax2.set_ylabel(\"total Aerosol Optical Thickness at 532 nm\") # set yaxis label \n",
        "plt.gcf().autofmt_xdate() # autoformat datetimes to look better\n",
        "\n",
        "ax1.set_title(\"Date: %s-%s-%s\"%(date[0],date[1],date[2])) #set title as flight date.\n",
        "\n",
        "# display and save figure using the *.ict data filename \n",
        "plt.savefig(\"%sHSRL-2&RSP_532nmAOT_timeseries_%s-%s-%s\"%(colab_path,HSRL2_Date[0],HSRL2_Date[1],HSRL2_Date[2]), dpi=300)\n",
        "plt.show() # function to display the plot"
      ],
      "metadata": {
        "id": "YU5N1R7eXlpG"
      },
      "id": "YU5N1R7eXlpG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10 - Generating remote sensing and in-situ aircraft flight track overlays\n",
        "\n",
        "#########################################################################################################################\n",
        "#\n",
        "# Description: procedure for plotting a flight tracks of both remote sensing and in-situ aircraft\n",
        "#\n",
        "#########################################################################################################################\n",
        "\n",
        "# generate arrays for the HSRL-2 latitude and longitude\n",
        "hsrl2_latitude_data = np.array(HSRL2_dictionary[\"Nav_Data\"][\"gps_lat\"])\n",
        "hsrl2_longitude_data = np.array(HSRL2_dictionary[\"Nav_Data\"][\"gps_lon\"])\n",
        "\n",
        "# create figure a geographical map for plotting flight tracks\n",
        "ax2=plt.subplots(1, 1,figsize=(12,12))\n",
        "\n",
        "# use Basemap function to display map of the research flight's study region and format\n",
        "m = Basemap(projection='cyl',llcrnrlat=np.floor(np.nanmin(latitude_data)),urcrnrlat=np.ceil(np.nanmax(latitude_data)),\\\n",
        "            llcrnrlon=np.floor(np.nanmin(longitude_data)),urcrnrlon=np.ceil(np.nanmax(longitude_data)),resolution='c')\n",
        "m.drawcoastlines() # diplay coastlines \n",
        "m.fillcontinents() # fill continents with grey \n",
        "\n",
        "# plot the flight track in latitude and longitude coordinates\n",
        "x,y = m(longitude_data,latitude_data)\n",
        "m.plot(x,y,'-ro',label='in-situ aircraft')\n",
        "x1,y1 = m(hsrl2_longitude_data,hsrl2_latitude_data)\n",
        "m.plot(x1,y1,'-bd',label='remote sensing aircraft')\n",
        "plt.legend(); # add legend\n",
        "\n",
        "# draw parallels and meridians\n",
        "m.drawparallels(np.arange(np.floor(np.nanmin(latitude_data)),np.ceil(np.nanmax(latitude_data)),1),labels=[1,1,0,1])\n",
        "m.drawmeridians(np.arange(np.floor(np.nanmin(longitude_data)),np.ceil(np.nanmax(longitude_data)),2),labels=[1,1,0,1])\n",
        "\n",
        "m.drawmapboundary(fill_color='#FFFFFF') # draw box around plot\n",
        "\n",
        "# format axes\n",
        "plt.xlabel('Longitude', labelpad=40)\n",
        "plt.ylabel('Latitude', labelpad=80)\n",
        "\n",
        "plt.title(\"Date: %s-%s-%s\"%(date[0],date[1],date[2])) #set title as flight date.\n",
        "plt.savefig(\"%sflight_tracks_%s-%s-%s\"%(colab_path,date[0],date[1],date[2]), dpi=300)\n",
        "plt.show() # function to show the plot"
      ],
      "metadata": {
        "id": "ZN8KYxrtca3F"
      },
      "id": "ZN8KYxrtca3F",
      "execution_count": null,
      "outputs": []
    }
  ]
}
